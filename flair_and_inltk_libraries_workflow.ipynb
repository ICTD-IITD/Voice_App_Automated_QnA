{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flair_and_inltk_libraries_workflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZI6SElOeBMX0kPrHQ8nzd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dragonsan17/faq_retrieval_deep_learning/blob/main/flair_and_inltk_libraries_workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXu6_nLPECRs"
      },
      "source": [
        "# Imports and Repo Downloading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RFKV9xNBHVa"
      },
      "source": [
        "!pip install torch==1.3.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install inltk\n",
        "!pip install flair\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('max_colwidth', 1000)\n",
        "from IPython.display import display\n",
        "from tqdm.notebook import tqdm\n",
        "import flair\n",
        "import warnings\n",
        "from flair.embeddings import FlairEmbeddings, DocumentRNNEmbeddings\n",
        "from flair.data import Sentence\n",
        "from sklearn.metrics.pairwise import cosine_similarity as cs\n",
        "from tqdm.notebook import tqdm\n",
        "from inltk.inltk import get_sentence_similarity\n",
        "warnings.filterwarnings('ignore') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VuknDu2BLOR"
      },
      "source": [
        "# Enter Username, Password and Repo name which will then download all the repo contents here in colab.\n",
        "# You can then access and run all files of the same. This is to get an idea of how the code works in a cloud environment\n",
        "# source : https://stackoverflow.com/questions/48350226/methods-for-using-git-with-google-colab\n",
        "user = input('User name: ')\n",
        "password = getpass('Password: ')\n",
        "password = urllib.parse.quote(password)\n",
        "\n",
        "# repo_name = input('Repo name: ')\n",
        "repo_name = 'faq_retrieval_deep_learning'\n",
        "cmd_string = 'git clone https://{0}:{1}@github.com/{0}/{2}.git'.format(user, password, repo_name)\n",
        "os.system(cmd_string)\n",
        "cmd_string, password = \"\", \"\" \n",
        "\n",
        "%cd faq_retrieval_deep_learning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecIb5HZ-EY6L"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPztO5b9BZqD"
      },
      "source": [
        "df_all_data = pd.read_csv('data/all_data.csv', encoding = 'utf-8')\n",
        "df_test = pd.read_csv('data/test.csv', encoding = 'utf-8')\n",
        "df_train = pd.read_csv('data/train.csv', encoding = 'utf-8')\n",
        "\n",
        "df_test['t1'] = [(list(df_all_data[df_all_data['STT Transcript'] == q]['Broad theme']) + list(df_all_data[df_all_data['Caller query transcription'] == q]['Broad theme']))[0] for q in list(df_test['q1']) ]\n",
        "df_test['t2'] = [(list(df_all_data[df_all_data['STT Transcript'] == q]['Broad theme']) + list(df_all_data[df_all_data['Caller query transcription'] == q]['Broad theme']))[0] for q in list(df_test['q2']) ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90xBL_TyEGuI"
      },
      "source": [
        "# Flair and iNLTK Models\n",
        "\n",
        "Take very long to predict on whole test dataset. Hence theme information is taken here itself and a score of 0 is given when theme is not same. Takes about an hour after this, for each of these models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BE-Yp_1KOsj"
      },
      "source": [
        "\"\"\"\n",
        "  Flair\n",
        "\"\"\"\n",
        "f_embedding = FlairEmbeddings('hi-forward')\n",
        "document_embeddings = DocumentRNNEmbeddings([f_embedding])\n",
        "\n",
        "def flair_docrnn(df_test):\n",
        "  test_q1 = df_test['q1']\n",
        "  test_q2 = df_test['q2']\n",
        "  predictions = []\n",
        "  dummy = np.arange(len(test_q1))\n",
        "  bar = tqdm(dummy)\n",
        "  for i in bar:\n",
        "    if df_test['t1'][i] != df_test['t2'][i]:\n",
        "      predictions.append(0)\n",
        "      continue\n",
        "    q1 = Sentence(test_q1[i])\n",
        "    q2 = Sentence(test_q2[i])\n",
        "    document_embeddings.embed(q1)\n",
        "    document_embeddings.embed(q2)\n",
        "    e1 = q1.embedding.cpu().detach().numpy().reshape(1, -1)\n",
        "    e2 = q2.embedding.cpu().detach().numpy().reshape(1, -1) \n",
        "    score = cs(e1,e2)[0][0]\n",
        "    predictions.append(score)\n",
        "\n",
        "  return predictions\n",
        "\n",
        "df_test['positive_score'] = flair_docrnn(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhq4NwOwKyI1"
      },
      "source": [
        "# # Run only once, to setup functions for Hindi in iNLTK\n",
        "# from inltk.inltk import setup\n",
        "# setup('hi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWUZFhlwKibw"
      },
      "source": [
        "# \"\"\"\n",
        "#   iNLTK\n",
        "# \"\"\"\n",
        "\n",
        "# def inltk_sentence_similarity(df_test):\n",
        "#   test_q1 = df_test['q1']\n",
        "#   test_q2 = df_test['q2']\n",
        "#   predictions = []\n",
        "#   dummy = np.arange(len(test_q1))\n",
        "#   bar = tqdm(dummy)\n",
        "#   for i in bar:\n",
        "#     if df_test['t1'][i] != df_test['t2'][i]:\n",
        "#       predictions.append(0)\n",
        "#       continue\n",
        "#     q1 = test_q1[i]\n",
        "#     q2 = test_q2[i]\n",
        "#     score = get_sentence_similarity(q1,q2, 'hi')\n",
        "#     predictions.append(score)\n",
        "\n",
        "#   return predictions\n",
        "\n",
        "# df_test['positive_score'] = inltk_sentence_similarity(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZM4IbG6EgMa"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW0A22_qBjGh"
      },
      "source": [
        "def performance_metric(df):\n",
        "\n",
        "  average_precision = 0\n",
        "  correct_answers = 0\n",
        "  success_rate = [0,0,0,0,0]\n",
        "  precision = [0,0,0,0,0]\n",
        "  reciprocal_rank = 0\n",
        "  \n",
        "  for index,row in df.iterrows():\n",
        "    query_question = row['q1']\n",
        "    predicted_question = row['q2']\n",
        "    \n",
        "    query_question_answer_index = list(df_all_data[df_all_data[TEST_COLUMN] == query_question]['Answer Index'])[0]\n",
        "    predicted_question_answer_index = list(df_all_data[df_all_data[TRAIN_COLUMN] == predicted_question]['Answer Index'])[0]\n",
        "\n",
        "    if query_question_answer_index == predicted_question_answer_index:\n",
        "      \n",
        "      correct_answers += 1\n",
        "      average_precision += correct_answers/(index + 1)\n",
        "      for i in range(index,5):\n",
        "        success_rate[i] = 1\n",
        "        precision[i] += 1/(i + 1)\n",
        "\n",
        "      if reciprocal_rank == 0:\n",
        "        reciprocal_rank = 1/(index + 1)\n",
        "\n",
        "  average_precision /= len(df)\n",
        "\n",
        "  calculated_metric = {'SR@1' : success_rate[0], 'SR@3' : success_rate[2], 'SR@5' : success_rate[4], \n",
        "                       'P@1' : precision[0], 'P@3' : precision[2], 'P@5' : precision[4],\n",
        "                       'MRR' : reciprocal_rank, 'MAP' : average_precision}\n",
        "  return calculated_metric\n",
        "  \n",
        "calculated_metric = {'SR@1' : 0, 'SR@3' : 0, 'SR@5' : 0, \n",
        "                      'P@1' : 0, 'P@3' : 0, 'P@5' : 0,\n",
        "                      'MRR' : 0, 'MAP' : 0}\n",
        "\n",
        "calculated_metric_with_themes = {'SR@1' : 0, 'SR@3' : 0, 'SR@5' : 0, \n",
        "                      'P@1' : 0, 'P@3' : 0, 'P@5' : 0,\n",
        "                      'MRR' : 0, 'MAP' : 0}\n",
        "\n",
        "query_question_groups = df_test.groupby(['q1'])\n",
        "\n",
        "for query_question in df_test['q1'].unique():\n",
        "    group = query_question_groups.get_group(query_question)\n",
        "    group['ai'] = [list(df_all_data[df_all_data[TRAIN_COLUMN] == ri]['Answer Index'])[0] for ri in list(group['q2'])]\n",
        "    ai_groups = group.groupby(['ai'])\n",
        "\n",
        "    for ans_i in group['ai'].unique():\n",
        "      group_ai = ai_groups.get_group(ans_i)\n",
        "      avg_score = group_ai['positive_score'].max() \n",
        "      group['positive_score'] = group.apply(lambda x: avg_score if x['ai'] == ans_i else x['positive_score'],  axis=1)\n",
        "    group = group.drop_duplicates(subset=['ai'])\n",
        "\n",
        "    \n",
        "    query_question_theme = list(df_all_data[df_all_data[TEST_COLUMN] == query_question][BROAD_THEME])[0]\n",
        "    group_with_themes = group.copy()\n",
        "    \n",
        "    for index, row in group_with_themes.iterrows():\n",
        "        if query_question_theme != list(df_all_data[df_all_data[TRAIN_COLUMN] == row['q2']][BROAD_THEME])[0]:\n",
        "            group_with_themes.loc[index, 'positive_score'] = 0\n",
        "    \n",
        "    group = group.sort_values(by=['positive_score'], ascending = False).reset_index(drop = True)\n",
        "    group_with_themes = group_with_themes.sort_values(by=['positive_score'], ascending = False).reset_index(drop = True)\n",
        "    group = group[group.index < 10]\n",
        "    group_with_themes = group_with_themes[group_with_themes.index < 10]\n",
        "    calculated_metric_for_group = performance_metric(group)\n",
        "    calculated_metric_for_group_with_themes = performance_metric(group_with_themes)\n",
        "\n",
        "    for key in calculated_metric_for_group:\n",
        "      calculated_metric[key] += calculated_metric_for_group[key]\n",
        "      calculated_metric_with_themes[key] += calculated_metric_for_group_with_themes[key]\n",
        "\n",
        "calculated_metric['Hit@1'] = calculated_metric['SR@1'] \n",
        "calculated_metric['Hit@3'] = calculated_metric['SR@3']\n",
        "calculated_metric['Hit@5'] = calculated_metric['SR@5']\n",
        "\n",
        "calculated_metric_with_themes['Hit@1'] = calculated_metric_with_themes['SR@1']\n",
        "calculated_metric_with_themes['Hit@3'] = calculated_metric_with_themes['SR@3']\n",
        "calculated_metric_with_themes['Hit@5'] = calculated_metric_with_themes['SR@5']\n",
        "\n",
        "for key in calculated_metric:\n",
        "  if 'Hit' not in key: \n",
        "    calculated_metric[key] /= len(query_question_groups)\n",
        "    calculated_metric_with_themes[key] /= len(query_question_groups)\n",
        "\n",
        "print(\"Results without theme information : \")\n",
        "print(\"Hit@1 : {}, 3: {}, 5 : {}, all : {}\".format(calculated_metric['Hit@1'], calculated_metric['Hit@3'], calculated_metric['Hit@5'], len(df_test['q1'].unique())))\n",
        "print(\"SR@1 : {:.3f}, 3: {:.3f}, 5 : {:.3f}\".format(calculated_metric['SR@1'], calculated_metric['SR@3'], calculated_metric['SR@5']))\n",
        "print(\"P@1 : {:.3f}, 3: {:.3f}, 5 : {:.3f}\".format(calculated_metric['P@1'], calculated_metric['P@3'], calculated_metric['P@5']))\n",
        "\n",
        "print(\"MAP : {:.3f}\".format(calculated_metric['MAP']), end=\", \")\n",
        "print(\"MRR : {:.3f}\".format(calculated_metric['MRR']))\n",
        "# print(\"NDCG : {:.3f}\".format(MDCG/deno_dd[\"Exist\"]))\n",
        "\n",
        "print(\"Results with theme information : \")\n",
        "print(\"Hit@1 : {}, 3: {}, 5 : {}, all : {}\".format(calculated_metric_with_themes['Hit@1'], calculated_metric_with_themes['Hit@3'], calculated_metric_with_themes['Hit@5'], len(df_test['q1'].unique())))\n",
        "print(\"SR@1 : {:.3f}, 3: {:.3f}, 5 : {:.3f}\".format(calculated_metric_with_themes['SR@1'], calculated_metric_with_themes['SR@3'], calculated_metric_with_themes['SR@5']))\n",
        "print(\"P@1 : {:.3f}, 3: {:.3f}, 5 : {:.3f}\".format(calculated_metric_with_themes['P@1'], calculated_metric_with_themes['P@3'], calculated_metric_with_themes['P@5']))\n",
        "\n",
        "print(\"MAP : {:.3f}\".format(calculated_metric_with_themes['MAP']), end=\", \")\n",
        "print(\"MRR : {:.3f}\".format(calculated_metric_with_themes['MRR']), end=\", \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94j2NdPhCYl8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}