{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_concatenation_augmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP5IheGfJAGlYCmJ33stVw9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dragonsan17/faq_retrieval_deep_learning/blob/main/data_concatenation_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXLugCd39YDU"
      },
      "source": [
        "# Data Concatenation and Augmentation Script\n",
        "\n",
        "This script first concatenates the required datasets (or can work on only 1, as specified in the list) and then generates similar sentences using 2 approaches: one is using iNLTK's library which has an api call for the same, and another is to use manually generated synonyms for key words in our dataset. The augmented data is then stored in the data directory, which can then be used in any of the workflow scripts. If there is no need for augmentation, save the train file after concatenation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzjtX3PZ95Ct"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiR5EMKoAwJQ"
      },
      "source": [
        "!pip install torch==1.3.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install inltk\n",
        "\n",
        "from inltk.inltk import get_similar_sentences\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M18mD8X_979C"
      },
      "source": [
        "## Data Pre-processing (concatenation, if required)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohQxSKyp_C-8"
      },
      "source": [
        "train_set = ['jee', 'kab', 'paj'] # Put all data here that has to be concatenated\n",
        "test_set = ['jee'] # Specify the test set here\n",
        "train_dfs = []\n",
        "test_dfs = []\n",
        "for data in train_set:\n",
        "    \n",
        "    df_all_data = pd.read_csv('./data/' + data + '_all_data.csv', encoding = 'utf-8')\n",
        "    df_train = pd.read_csv('./data' + data + '_train.csv', encoding = 'utf-8')\n",
        "    if data == 'paj':\n",
        "        df_all_data['Relevant Point'] = df_all_data['Answer Transcription']\n",
        "\n",
        "    if data != 'kab':\n",
        "        df_train['a2'] = [(list(df_all_data[df_all_data['STT Transcript'] == q]['Relevant Point']) + list(df_all_data[df_all_data['Caller query transcription'] == q]['Relevant Point']))[0] for q in list(df_train['q2']) ]\n",
        "        df_train['q2_r'] = [(list(df_all_data[df_all_data['STT Transcript'] == q]['Relevant Topic']) + list(df_all_data[df_all_data['Caller query transcription'] == q]['Relevant Topic']))[0] for q in list(df_train['q2']) ]\n",
        "    else:\n",
        "        df_train['a2'] = [(list(df_all_data[df_all_data['Caller query transcription'] == q]['Relevant Point']))[0] for q in list(df_train['q2']) ]\n",
        "        df_train['q2_r'] = [(list(df_all_data[df_all_data['Caller query transcription'] == q]['Relevant Topic']))[0] for q in list(df_train['q2']) ]\n",
        "    \n",
        "    \n",
        "    train_dfs.append(df_train)\n",
        "  \n",
        "df_train = pd.concat(train_dfs) \n",
        "df_train = df_train.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFmq8VyA-o61"
      },
      "source": [
        "\"\"\"\n",
        "  Uncomment and save if only concatenated data is needed.\n",
        "\"\"\"\n",
        "# df_train.to_csv('./data/concat_train.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2yZuAbUBNQy"
      },
      "source": [
        "df_syn = pd.read_excel('./data/synonyms.xlsx', sheet_name=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdNjdR_C-CPo"
      },
      "source": [
        "## Similar Sentences Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA7MkeBWAsM1"
      },
      "source": [
        "def main(sent, synonyms, word = None):\n",
        "    out_sents = []\n",
        "\n",
        "    if word != None:\n",
        "      # Preprocess synonyms\n",
        "      sent = sent.strip()\n",
        "      # synonyms = '{}, {}'.format(word, synonyms)\n",
        "      print(synonyms)\n",
        "      synonyms = synonyms.split(',')\n",
        "      synonyms = [syn.strip() for syn in synonyms]\n",
        "      # print('Synonyms: {}'.format(synonyms))\n",
        "      # Get the appended list\n",
        "      for syn in (synonyms):\n",
        "          new_sent = sent.replace(word, syn)\n",
        "          out_sents.append(new_sent)\n",
        "\n",
        "    syn_sents = get_similar_sentences(sent, 2, 'hi', 0.3)\n",
        "    out_sents.extend(syn_sents)\n",
        "    print(out_sents)\n",
        "    return out_sents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9ohZQq4CbfX"
      },
      "source": [
        "# Run only once, to setup functions for Hindi in iNLTK\n",
        "from inltk.inltk import setup\n",
        "setup('hi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJu8ReWeC8wa"
      },
      "source": [
        "import random\n",
        "\n",
        "q1 = []\n",
        "q2 = []\n",
        "label = []\n",
        "a2 = []\n",
        "q2_r = []\n",
        "\n",
        "words = df_syn['Word']\n",
        "print(len(df_train))\n",
        "for i,r in df_train.iterrows():\n",
        "  \n",
        "  q1_list = [r['q1']]\n",
        "  for word in words:\n",
        "    if word in r['q1']:\n",
        "      q1_list.extend(main(r['q1'], list(df_syn[df_syn['Word'].str.contains(word)]['Synonyms'])[0], word))\n",
        "\n",
        "  q2_list = [r['q2']]\n",
        "  for word in words:\n",
        "    if word in r['q2']:\n",
        "      q1_list.extend(main(r['q2'], list(df_syn[df_syn['Word'].str.contains(word)]['Synonyms'])[0], word))\n",
        "\n",
        "  if len(q1_list) == 1 and len(q2_list) == 1:\n",
        "    continue \n",
        "  a2_list = [r['a2']]\n",
        "  for word in words:\n",
        "    if word in r['a2']:\n",
        "      q1_list.extend(main(r['a2'], list(df_syn[df_syn['Word'].str.contains(word)]['Synonyms'])[0], word))\n",
        "\n",
        "  q2_r_list = [r['q2_r']]\n",
        "  for word in words:\n",
        "    if word in r['q2_r']:\n",
        "      q1_list.extend(main(r['q2_r'], list(df_syn[df_syn['Word'].str.contains(word)]['Synonyms'])[0], word))\n",
        "      \n",
        "  for j in range(5):\n",
        "    q1.append(random.choice(q1_list))\n",
        "    q2.append(random.choice(q2_list))\n",
        "    label.append(r['label'])\n",
        "    a2.append(random.choice(a2_list))\n",
        "    q2_r.append(random.choice(q2_r_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvbt1sl7-L5X"
      },
      "source": [
        "## Saving Augmented data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pq9GssUGpD7"
      },
      "source": [
        "df_new_train = pd.DataFrame({'q1' : q1, 'q2' : q2, 'label' : label, 'a2' : a2, 'q2_r' : q2_r})\n",
        "\n",
        "df_new_train.to_csv('./data/aug_train.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}