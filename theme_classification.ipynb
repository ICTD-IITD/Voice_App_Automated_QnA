{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "theme_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOcBIyG/ZUTPRZM1k7CXp77",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dragonsan17/faq_retrieval_deep_learning/blob/main/theme_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76E31ntlGs8J"
      },
      "source": [
        "# Theme Classification\n",
        "\n",
        "Here, we experiment on automatic theme classification, using BERT and Tf-Idf Weighted N-Gram models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_RTA6kUG185"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyiwF7xNBhMP"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('max_colwidth', 1000)\n",
        "from IPython.display import display\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') \n",
        "\n",
        "from transformers import TFBertModel, BertTokenizer, TFAutoModel, AutoTokenizer\n",
        "import random\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.metrics import f1_score as f1\n",
        "from sklearn.metrics import accuracy_score as acc\n",
        "from sklearn.metrics import classification_report as report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTVZhOhZG4Gm"
      },
      "source": [
        "## Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXSo8YykBliQ"
      },
      "source": [
        "\"\"\"\n",
        "Here it is assumed that the training data has Caller query transcription, hence we are matching that\n",
        "with all_data.csv file and then picking out broad theme, which is the label. Here, we train with\n",
        "Relevant Topic (we can also experiment with Caller query transcription itself). \n",
        "\"\"\"\n",
        "data = 'paj' #mention the dataset on which theme classification is to be performed\n",
        "\n",
        "df_all_data = pd.read_csv('.data/' + data + '_all_data.csv', encoding = 'utf-8')\n",
        "df_train = pd.read_csv('.data/' + data + '_train.csv')\n",
        "\n",
        "x_train_ = list(set(list(df_train['q1']) + list(df_train['q2'])))\n",
        "x_train = []\n",
        "y_train_labels = []\n",
        "for i in range(len(x_train_)):\n",
        "    \n",
        "    l = list(df_all_data[df_all_data['Caller query transcription'] == x_train_[i]]['Broad theme'])\n",
        "    if len(l) > 0:\n",
        "        x_train.append(list(df_all_data[df_all_data['Caller query transcription'] == x_train_[i]]['Relevant Topic'])[0])\n",
        "        y_train_labels.append(l[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuVfLzOpCXJ-"
      },
      "source": [
        "\"\"\"\n",
        "We test here on the test set of corresponding dataset, or split train into train and val\n",
        "and then import val here if we are doing hyperparameter tuning. Make sure to use \n",
        "'STT Transcript' while validating or testing as that is what we receive in real world scenario\n",
        "\"\"\"\n",
        "df_val = pd.read_csv('.data/' + data + '_test.csv')\n",
        "x_val_ = list(set(list(df_val['q1'])))\n",
        "x_val = []\n",
        "y_val_labels = []\n",
        "for i in range(len(x_val_)):\n",
        "    \n",
        "    l = list(df_all_data[df_all_data['STT Transcript'] == x_val_[i]]['Broad theme'])\n",
        "    if len(l) > 0:\n",
        "        x_val.append(list(df_all_data[df_all_data['STT Transcript'] == x_val_[i]]['STT Transcript'])[0])\n",
        "        y_val_labels.append(l[0])\n",
        "    else:\n",
        "        print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOINNp3kEbOq"
      },
      "source": [
        "\"\"\"\n",
        "Data split, if needed for tuning hyperparameters.\n",
        "\"\"\"\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# df_train, df_val, _, _ = train_test_split(df_all_data, df_all_data['Broad theme'], test_size=0.3, random_state=42, stratify=df_all_data['Broad theme'])\n",
        "# df_train = df_train.reset_index()\n",
        "# df_val = df_val.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs1WTbwBCxQB"
      },
      "source": [
        "## N-Gram Models\n",
        "\n",
        "All the models here consider tf-idf weighted inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK3OwJZpCXUH"
      },
      "source": [
        "from IPython.display import display\n",
        "from collections import Counter\n",
        "def computeTF(wordDict, bagOfWords):\n",
        "    tfDict = {}\n",
        "    bagOfWordsCount = len(bagOfWords)\n",
        "    for word, count in wordDict.items():\n",
        "        tfDict[word] = count / float(bagOfWordsCount)\n",
        "    return tfDict\n",
        "\n",
        "def computeIDF(unique_words, documents):\n",
        "    import math\n",
        "    N = len(documents)\n",
        "    \n",
        "    idfDict = dict.fromkeys(unique_words, 0)\n",
        "    for document in documents:\n",
        "        for word in document:\n",
        "            idfDict[word] += 1\n",
        "    \n",
        "    for word, val in idfDict.items():\n",
        "\n",
        "        idfDict[word] = math.log(N / float(val))\n",
        "    return idfDict\n",
        "\n",
        "def computeTFIDF(tfBagOfWords, idfs):\n",
        "    tfidf = {}\n",
        "    for word, val in tfBagOfWords.items():\n",
        "        tfidf[word] = val * idfs[word]\n",
        "    return tfidf\n",
        "    \n",
        "\n",
        "stopwords = []\n",
        "labels = list(set(list(df_train['Broad theme'])))\n",
        "\n",
        "documents = list(df_train['Relevant Topic'])\n",
        "\n",
        "for i in range(len(documents)):\n",
        "    documents[i] = ' '.join(documents[i].split('?'))\n",
        "    \n",
        "train_text = documents\n",
        "print(len(documents))\n",
        "\n",
        "bag_of_words = []\n",
        "unique_words = set([])\n",
        "for document in documents:\n",
        "    new_words = set(document.split())\n",
        "    #     print(len(new_words))\n",
        "    unique_words = unique_words.union(new_words)\n",
        "    bag_of_words.append(new_words)\n",
        "\n",
        "unique_words.discard(set(stopwords))\n",
        "print(len(unique_words))\n",
        "num_words_all = []\n",
        "\n",
        "for i in range(len(bag_of_words)):\n",
        "    num_words = dict.fromkeys(unique_words, 0)\n",
        "    for word in documents[i].split():\n",
        "        num_words[word] += 1\n",
        "    num_words_all.append(num_words)\n",
        "    \n",
        "\n",
        "idf = computeIDF(unique_words, bag_of_words)\n",
        "\n",
        "all_tfidf = []\n",
        "for i in range(len(bag_of_words)):\n",
        "    tf = computeTF(num_words_all[i], bag_of_words[i])\n",
        "    tfidf = computeTFIDF(tf, idf)\n",
        "    all_tfidf.append(tfidf)\n",
        "\n",
        "df_t  = pd.DataFrame(all_tfidf)\n",
        "\n",
        "documents = list(df_val['Relevant Topic']) \n",
        "# documents = list(df_val['STT Transcript'])\n",
        "bag_of_words = []\n",
        "for document in documents:\n",
        "    new_words = set(document.split())\n",
        "    bag_of_words.append(new_words)\n",
        "    \n",
        "num_words_all = []\n",
        "\n",
        "for i in range(len(bag_of_words)):\n",
        "    num_words = dict.fromkeys(unique_words, 0)\n",
        "    for word in documents[i].split():\n",
        "        if word in num_words:\n",
        "            num_words[word] += 1\n",
        "    num_words_all.append(num_words)\n",
        "  \n",
        "all_tfidf = []\n",
        "for i in range(len(bag_of_words)):\n",
        "    tf = computeTF(num_words_all[i], bag_of_words[i])\n",
        "    tfidf = computeTFIDF(tf, idf)\n",
        "    all_tfidf.append(tfidf)\n",
        "    \n",
        "df_v= pd.DataFrame(all_tfidf)\n",
        "   \n",
        "y_train = df_train['Broad theme']\n",
        "y_val = df_val['Broad theme']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHgF6fo7CXW6"
      },
      "source": [
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model_rf = RandomForestClassifier(n_jobs = 10, n_estimators = 1000, min_samples_split = 2, class_weight = 'balanced')\n",
        "model_dt = DecisionTreeClassifier(class_weight = 'balanced')\n",
        "model_gb = GradientBoostingClassifier(n_estimators = 200, min_samples_split = 3, max_depth = 9)\n",
        "model_lr = LogisticRegression(max_iter = 1000, penalty = 'l2', class_weight = 'balanced', C = 3)\n",
        "model_svc = LinearSVC(C = 2)\n",
        "\n",
        "model = model_rf #specify the model here\n",
        "model.fit(np.array(df_t), y_train)\n",
        "y_train_p = model.predict(np.array(df_t))\n",
        "y_val_p = model.predict(np.array(df_v))\n",
        "print(report(y_val, y_val_p))\n",
        "print(acc(y_val, y_val_p))\n",
        "print(f1(y_val, y_val_p, average = 'macro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN-XPx6zB6QB"
      },
      "source": [
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "array = confusion_matrix(y_val, y_val_p, labels = df_train['Broad theme'].unique())\n",
        "df_cm = pd.DataFrame(array, index = [i for i in df_train['Broad theme'].unique()],\n",
        "                  columns = [i for i in df_train['Broad theme'].unique()])\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXI2H-3-C1KW"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zH8qsKYC55m"
      },
      "source": [
        "all_themes_ = list(df_all_data['Broad theme'].unique())\n",
        "all_themes = {all_themes_[i] : i for i in range(len(all_themes_))}\n",
        "\n",
        "y_train_ = [all_themes[label] for label in y_train_labels]\n",
        "y_train = tf.one_hot(y_train_, len(all_themes))\n",
        "\n",
        "y_val_ = [all_themes[label] for label in y_val_labels]\n",
        "y_val = tf.one_hot(y_val_, len(all_themes))\n",
        "\n",
        "def preprocess_text(tokenizer, ques):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for q1 in ques:\n",
        "        q1 = '[CLS] ' + q1 + ' [SEP] '\n",
        "        token = tokenizer.tokenize(q1)\n",
        "        attention_mask = [1]*len(token)\n",
        "\n",
        "        input_id = tokenizer.convert_tokens_to_ids(token)\n",
        "\n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "\n",
        "    input_ids = np.array(pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\"))  \n",
        "    attention_masks = np.array(pad_sequences(attention_masks, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")) \n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0stOZnNwDIHe"
      },
      "source": [
        "\"\"\"\n",
        "  BERT\n",
        "\"\"\"\n",
        "\n",
        "def build_model():\n",
        "    \n",
        "    input_ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    attention_masks = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "\n",
        "    bert_model = TFBertModel.from_pretrained(pretrained_model_name_or_path = 'bert-base-multilingual-cased', return_dict=True)\n",
        "    x = bert_model(input_ids,attention_mask=attention_masks).pooler_output\n",
        "    x1 = tf.keras.layers.Dropout(0.1)(x) \n",
        "    x1 = tf.keras.layers.Dense(len(all_themes))(x1)\n",
        "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[input_ids, attention_masks], outputs=[x1])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate= LR)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
        "    return model, BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pgBi-4FDJYN"
      },
      "source": [
        "# BERT On PAJ\n",
        "MAX_LEN = 512\n",
        "LR = 2e-5\n",
        "EPOCH_NUM = 15\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "#BERT On JEE\n",
        "# MAX_LEN = 256\n",
        "# LR = 6e-6\n",
        "# EPOCH_NUM = 10\n",
        "# BATCH_SIZE = 4\n",
        "\n",
        "model, tokenizer = build_model()\n",
        "\n",
        "train_input_ids, train_attention_masks = preprocess_text(tokenizer, x_train)\n",
        "val_input_ids, val_attention_masks = preprocess_text(tokenizer, x_val)\n",
        "\n",
        "model.fit([train_input_ids, train_attention_masks], y_train, epochs=EPOCH_NUM, batch_size=BATCH_SIZE, verbose=1, validation_data=([val_input_ids, val_attention_masks], y_val))\n",
        "\n",
        "y_pred = model.predict([val_input_ids, val_attention_masks])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXEQLg36DoYV"
      },
      "source": [
        "## Saving Test Data\n",
        "\n",
        "We modify the test set as follows: We consider the top 3 themes predicted for the test query q2, and then only consider the pairs (q1,q2), where the q1's theme is one of the top 3 predicted themes for q2. We then test on this modified test set, which is equivalent to filtering out queries and then testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuAUau8rDd1r"
      },
      "source": [
        "#use y_val_p instead of y_pred if using N gram models\n",
        "\n",
        "all_themes_ = list(df_all_data['Broad theme'].unique())\n",
        "all_themes = {all_themes_[i] : i for i in range(len(all_themes_))}\n",
        "\n",
        "q1 = []\n",
        "q2 = []\n",
        "for i,r in df_val.iterrows():\n",
        "    q = r['q1']\n",
        "    if q in x_val:\n",
        "        id_ = x_val.index(q)\n",
        "        preds = np.argsort(y_pred[id_])[-3:]\n",
        "        s = set([all_themes_[preds[-3]], all_themes_[preds[-2]], all_themes_[preds[-1]]])\n",
        "        l = list(df_all_data[df_all_data['Caller query transcription'] == r['q2']]['Broad theme'])\n",
        "        if len(l) == 0:\n",
        "            continue\n",
        "            \n",
        "        t2 = l[0]\n",
        "        if t2 in s:\n",
        "            q1.append(r['q1'])\n",
        "            q2.append(r['q2'])\n",
        "\n",
        "df = pd.DataFrame({'q1' : q1, 'q2' : q2})\n",
        "df.to_csv(data + '_test_themes.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}